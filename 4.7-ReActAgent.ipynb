{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a211dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun,ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402437e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4996184",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_wikipedia = WikipediaAPIWrapper(top_k_results=3,doc_content_chars_max=1000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d555772",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.invoke(\"What is Generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c80620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"ReAct_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function\n",
    "# Doc string inside the functin is important to write because it help to understand the work of the tool/function by the LLM \n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\" Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\" Add a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a+b\n",
    "\n",
    "def divide(a:int, b:int) -> float:\n",
    "    \"\"\" Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a/b\n",
    "tools = [arxiv,wiki,add,multiply,divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ca4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tavily seacrh tool\n",
    "from langchain_tavily import TavilySearch\n",
    "tavily = TavilySearch(max_result=3,topic='general',include_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily.invoke(\"Give me latest news about AI within ongoing month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [arxiv,wiki,tavily,add,multiply,divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize my LLm Model\n",
    "from langchain_groq import  ChatGroq\n",
    "llm = ChatGroq(model='llama-3.1-8b-instant')\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54885d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "llm_with_tools.invoke([HumanMessage(content=\"What is the recent AI news\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Schema\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "class State(TypedDict):\n",
    "    messages : Annotated[list[AnyMessage],add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the graph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image,display\n",
    "\n",
    "# node definition\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "#add node\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "# adding edges\n",
    "builder.add_edge(START,'tool_calling_llm')\n",
    "builder.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "builder.add_edge('tools',\"tool_calling_llm\")\n",
    "\n",
    "graph=builder.compile()\n",
    "\n",
    "#view\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = graph.invoke({\"messages\":HumanMessage(content=\"Provide me the top 5 AI recent news from september 10 2025, and add 5 plus 4 and then divide the result by 3\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149682e",
   "metadata": {},
   "source": [
    "### GRaph with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the graph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image,display\n",
    "\n",
    "# node definition\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "#add node\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "# adding edges\n",
    "builder.add_edge(START,'tool_calling_llm')\n",
    "builder.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "builder.add_edge('tools',\"tool_calling_llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba528d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph_memory=builder.compile(checkpointer=memory)\n",
    "\n",
    "#view\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2338a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the thread\n",
    "config = {\"configurable\": {\"thread_id\":1}}\n",
    "# specify the input\n",
    "messages = [HumanMessage(content=\"Add 12 and 13\")]\n",
    "messages = graph_memory.invoke({\"messages\":messages},config=config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add that number to 25\")]\n",
    "messages = graph_memory.invoke({\"messages\":messages},config=config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4651860",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"then multiply that number by 2\")]\n",
    "messages = graph_memory.invoke({\"messages\":messages},config=config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ab251",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is 4 plus 5\")]\n",
    "messages = graph_memory.invoke({\"messages\":messages},config=config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
